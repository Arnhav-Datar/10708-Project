INFO:root:Namespace(lm_model='bert-base-uncased', N=50, max_len=128, z_dim=8, mha_dim=768, n_heads=8, hid_dims=[128, 256, 768], hid_dims_2=[512, 256, 128], m_dim=128, conv_dim=[[128, 256], 768, [256, 64]], lambda_rec=10, lambda_gp=10.0, post_method='sigmoid', batch_size=128, num_epochs=100, g_lr=0.0001, d_lr=0.0001, dropout=0.0, n_critic=5, resume_epoch=None, test_epochs=100, num_workers=1, mode='train', data_dir='data', saving_dir='results/2023-04-05', log_step=1, sample_step=1000, model_save_step=20, lr_update_step=1000, lambda_wgan=1, log_dir='results/2023-04-05/logs', model_dir='results/2023-04-05/models')
INFO:root:Generator(
  (activation_f): Tanh()
  (multi_dense_layer): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=8, out_features=128, bias=True)
      (1): Dropout(p=0.0, inplace=False)
      (2): Tanh()
      (3): Linear(in_features=128, out_features=256, bias=True)
      (4): Dropout(p=0.0, inplace=False)
      (5): Tanh()
      (6): Linear(in_features=256, out_features=768, bias=True)
      (7): Dropout(p=0.0, inplace=False)
      (8): Tanh()
    )
  )
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (multi_dense_layer_2): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=768, out_features=512, bias=True)
      (1): Dropout(p=0.0, inplace=False)
      (2): Tanh()
      (3): Linear(in_features=512, out_features=256, bias=True)
      (4): Dropout(p=0.0, inplace=False)
      (5): Tanh()
      (6): Linear(in_features=256, out_features=128, bias=True)
      (7): Dropout(p=0.0, inplace=False)
      (8): Tanh()
    )
  )
  (adjM_layer): Linear(in_features=128, out_features=2500, bias=True)
  (dropoout): Dropout(p=0.0, inplace=False)
)
INFO:root:G
INFO:root:The number of parameters: 3474372
INFO:root:Discriminator(
  (activation_f): Tanh()
  (gcn_layer): GraphConvolution(
    (activation_f): Tanh()
    (multi_graph_convolution_layers): MultiGraphConvolutionLayers(
      (conv_nets): ModuleList(
        (0): GraphConvolutionLayer(
          (adj_list): Linear(in_features=128, out_features=128, bias=True)
          (linear_2): Linear(in_features=128, out_features=128, bias=True)
          (activation): Tanh()
          (dropout): Dropout(p=0.0, inplace=False)
          (pe): PositionalEncoder()
        )
        (1): GraphConvolutionLayer(
          (adj_list): Linear(in_features=128, out_features=256, bias=True)
          (linear_2): Linear(in_features=128, out_features=256, bias=True)
          (activation): Tanh()
          (dropout): Dropout(p=0.0, inplace=False)
          (pe): PositionalEncoder()
        )
      )
    )
  )
  (agg_layer): GraphAggregation(
    (activation): Tanh()
    (i): Sequential(
      (0): Linear(in_features=256, out_features=768, bias=True)
      (1): Sigmoid()
    )
    (j): Sequential(
      (0): Linear(in_features=256, out_features=768, bias=True)
      (1): Tanh()
    )
    (dropout): Dropout(p=0.0, inplace=False)
    (pe): PositionalEncoder()
  )
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (multi_dense_layer): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=768, out_features=256, bias=True)
      (1): Dropout(p=0.0, inplace=False)
      (2): Tanh()
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.0, inplace=False)
      (5): Tanh()
    )
  )
  (output_layer): Linear(in_features=64, out_features=1, bias=True)
)
INFO:root:D
INFO:root:The number of parameters: 3069569
INFO:root:BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0-11): 12 x BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
INFO:root:bert-base-uncased
INFO:root:The number of parameters: 109482240
INFO:root:Elapsed [0:05:24], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:05:36], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:05:47], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:05:59], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:06:11], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:06:23], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:06:35], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:06:47], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:06:59], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:07:10], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:07:21], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:07:33], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:07:45], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:07:56], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:08:08], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:08:19], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:08:31], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:08:43], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:08:55], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
INFO:root:Elapsed [0:09:06], Iteration [1/100]:
l_D/R: nan, l_D/F: nan, l_D: nan, l_G: nan
property_match: 0.20
