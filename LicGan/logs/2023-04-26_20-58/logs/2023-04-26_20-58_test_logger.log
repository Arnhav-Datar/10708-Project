INFO:root:Namespace(lm_model='roberta-base', N=50, max_len=128, z_dim=8, mha_dim=768, n_heads=8, gen_dims=[[128, 256, 768], [512, 512]], disc_dims=[[128, 128], [512, 768], [512, 256, 128]], lambda_gp=5, lambda_rew=0.5, lambda_wgan=1, post_method='hard_gumbel', batch_size=128, num_epochs=100, g_lr=0.0002, d_lr=0.0002, b_lr=1e-05, dropout=0, n_critic=4, test_epochs=100, num_workers=1, mode='test', bert_unfreeze=0, data_dir='data/graphgen', saving_dir='results/2023-04-26_20-58', model_save_step=20, lr_update_step=1000, restore_G='/home/abisheks/10708-Project/MolGAN-PyTorch/results/2023-04-23_22-39/models/60-G.ckpt', restore_D='/home/abisheks/10708-Project/MolGAN-PyTorch/results/2023-04-23_22-39/models/60-D.ckpt', restore_B_D=None, restore_B_G=None, model_mode=1, ds_mode=0, name='symm_fcn_final_ds_any_prop_gumbel_roberta_m1_mask', test_category_wise=1, log_dir='results/2023-04-26_20-58/logs', model_dir='results/2023-04-26_20-58/models')
INFO:root:Generator(
  (activation_f): ReLU()
  (multi_dense_layer): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=8, out_features=128, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=256, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=768, bias=True)
      (9): Dropout(p=0, inplace=False)
      (10): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (11): ReLU()
    )
  )
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (multi_dense_layer_2): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=1536, out_features=512, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=512, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
    )
  )
  (adjM_layer): Sequential(
    (0): Linear(in_features=512, out_features=128, bias=True)
    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (2): ReLU()
    (3): Linear(in_features=128, out_features=2500, bias=True)
  )
  (node_layer): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (2): ReLU()
    (3): Linear(in_features=64, out_features=50, bias=True)
  )
)
INFO:root:G
INFO:root:The number of parameters: 4072502
INFO:root:Discriminator(
  (activation_f): ReLU()
  (multi_dense_layer): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=50, out_features=128, bias=True)
      (1): Dropout(p=0.0, inplace=False)
      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Dropout(p=0.0, inplace=False)
      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
    )
  )
  (node_dense_layer): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=50, out_features=64, bias=True)
      (1): Dropout(p=0.2, inplace=False)
      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=64, out_features=128, bias=True)
      (5): Dropout(p=0.2, inplace=False)
      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
    )
  )
  (multi_dense_layer_2): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=6528, out_features=512, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=768, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
    )
  )
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (multi_dense_layer_3): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=1536, out_features=512, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=256, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=128, bias=True)
      (9): Dropout(p=0, inplace=False)
      (10): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (11): ReLU()
    )
  )
  (output_layer): Linear(in_features=128, out_features=1, bias=True)
)
INFO:root:D
INFO:root:The number of parameters: 7090369
INFO:root:RobertaModel(
  (embeddings): RobertaEmbeddings(
    (word_embeddings): Embedding(50265, 768, padding_idx=1)
    (position_embeddings): Embedding(514, 768, padding_idx=1)
    (token_type_embeddings): Embedding(1, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): RobertaEncoder(
    (layer): ModuleList(
      (0-11): 12 x RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): RobertaPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
INFO:root:roberta-base_G
INFO:root:The number of parameters: 124645632
INFO:root:RobertaModel(
  (embeddings): RobertaEmbeddings(
    (word_embeddings): Embedding(50265, 768, padding_idx=1)
    (position_embeddings): Embedding(514, 768, padding_idx=1)
    (token_type_embeddings): Embedding(1, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): RobertaEncoder(
    (layer): ModuleList(
      (0-11): 12 x RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): RobertaPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
INFO:root:roberta-base_D
INFO:root:The number of parameters: 124645632
INFO:root:5 sample adjacenecy matrices
--------------------------------------------------
Text: Undirected graph with with cycle, max diameter 2.
Results: [49, 9, 0, 4, 2, 44, True]
--------------------------------------------------
--------------------------------------------------
Text: Undirected graph with min degree 1, 5 nodes, 2 connected component, 3 edges, without cycle, max diameter 2, max degree 2.
Results: [5, 1, 0, 1, 2, 4, False]
--------------------------------------------------
--------------------------------------------------
Text: Undirected graph with 22 nodes, max degree 9, max diameter 4.
Results: [22, 81, 0, 11, 3, 1, True]
--------------------------------------------------
--------------------------------------------------
Text: Undirected graph with min degree 1, max diameter 5.
Results: [50, 19, 0, 5, 6, 37, True]
--------------------------------------------------
--------------------------------------------------
Text: Undirected graph with max diameter 5, 1 connected component, max degree 11, 23 nodes.
Results: [24, 84, 0, 11, 3, 1, True]
--------------------------------------------------

INFO:root:Elapsed [0:00:10], Iteration [1/100]:
l_D/R: 9.69, l_D/F: 2.55, l_D: -5.05, l_G: -2.65, l_R: 0.00, l_R/N: 0.00, l_R/M: 0.00
property_match: 0.31, closeness: 0.44, n_match: 0.33, m_match: 0.06, min_deg_match: 0.01, max_deg_match: 0.24, diam_match: 0.26, cc_match: 0.27, cycle_match: 0.86
--------------------------------------------------
Nodes in (0, 5]: 25
property_match: 0.42, closeness: 0.59, n_match: 0.72, m_match: 0.08, min_deg_match: 0.00, max_deg_match: 0.27, diam_match: 0.53, cc_match: 0.42, cycle_match: 0.67
Nodes in (5, 10]: 86
property_match: 0.42, closeness: 0.58, n_match: 0.81, m_match: 0.15, min_deg_match: 0.00, max_deg_match: 0.34, diam_match: 0.32, cc_match: 0.46, cycle_match: 0.73
Nodes in (10, 25]: 94
property_match: 0.35, closeness: 0.47, n_match: 0.43, m_match: 0.04, min_deg_match: 0.02, max_deg_match: 0.22, diam_match: 0.31, cc_match: 0.52, cycle_match: 0.92
Nodes in (25, 50]: 295
property_match: 0.26, closeness: 0.38, n_match: 0.13, m_match: 0.04, min_deg_match: 0.02, max_deg_match: 0.21, diam_match: 0.19, cc_match: 0.08, cycle_match: 0.89
--------------------------------------------------
INFO:root:Namespace(lm_model='roberta-base', N=50, max_len=128, z_dim=8, mha_dim=768, n_heads=8, gen_dims=[[128, 256, 768], [512, 512]], disc_dims=[[128, 128], [512, 768], [512, 256, 128]], lambda_gp=5, lambda_rew=0.5, lambda_wgan=1, post_method='hard_gumbel', batch_size=128, num_epochs=100, g_lr=0.0002, d_lr=0.0002, b_lr=1e-05, dropout=0, n_critic=4, test_epochs=100, num_workers=1, mode='test', bert_unfreeze=0, data_dir='data/graphgen', saving_dir='results/2023-04-26_20-58', model_save_step=20, lr_update_step=1000, restore_G='/home/abisheks/10708-Project/MolGAN-PyTorch/results/2023-04-23_22-39/models/80-G.ckpt', restore_D='/home/abisheks/10708-Project/MolGAN-PyTorch/results/2023-04-23_22-39/models/80-D.ckpt', restore_B_D=None, restore_B_G=None, model_mode=1, ds_mode=0, name='symm_fcn_final_ds_any_prop_gumbel_roberta_m1_mask', test_category_wise=1, log_dir='results/2023-04-26_20-58/logs', model_dir='results/2023-04-26_20-58/models')
INFO:root:Generator(
  (activation_f): ReLU()
  (multi_dense_layer): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=8, out_features=128, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=256, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=768, bias=True)
      (9): Dropout(p=0, inplace=False)
      (10): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (11): ReLU()
    )
  )
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (multi_dense_layer_2): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=1536, out_features=512, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=512, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
    )
  )
  (adjM_layer): Sequential(
    (0): Linear(in_features=512, out_features=128, bias=True)
    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (2): ReLU()
    (3): Linear(in_features=128, out_features=2500, bias=True)
  )
  (node_layer): Sequential(
    (0): Linear(in_features=512, out_features=64, bias=True)
    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (2): ReLU()
    (3): Linear(in_features=64, out_features=50, bias=True)
  )
)
INFO:root:G
INFO:root:The number of parameters: 4072502
INFO:root:Discriminator(
  (activation_f): ReLU()
  (multi_dense_layer): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=50, out_features=128, bias=True)
      (1): Dropout(p=0.0, inplace=False)
      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Dropout(p=0.0, inplace=False)
      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
    )
  )
  (node_dense_layer): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=50, out_features=64, bias=True)
      (1): Dropout(p=0.2, inplace=False)
      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=64, out_features=128, bias=True)
      (5): Dropout(p=0.2, inplace=False)
      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
    )
  )
  (multi_dense_layer_2): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=6528, out_features=512, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=768, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
    )
  )
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (multi_dense_layer_3): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=1536, out_features=512, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=256, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=128, bias=True)
      (9): Dropout(p=0, inplace=False)
      (10): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (11): ReLU()
    )
  )
  (output_layer): Linear(in_features=128, out_features=1, bias=True)
)
INFO:root:D
INFO:root:The number of parameters: 7090369
INFO:root:RobertaModel(
  (embeddings): RobertaEmbeddings(
    (word_embeddings): Embedding(50265, 768, padding_idx=1)
    (position_embeddings): Embedding(514, 768, padding_idx=1)
    (token_type_embeddings): Embedding(1, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): RobertaEncoder(
    (layer): ModuleList(
      (0-11): 12 x RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): RobertaPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
INFO:root:roberta-base_G
INFO:root:The number of parameters: 124645632
INFO:root:RobertaModel(
  (embeddings): RobertaEmbeddings(
    (word_embeddings): Embedding(50265, 768, padding_idx=1)
    (position_embeddings): Embedding(514, 768, padding_idx=1)
    (token_type_embeddings): Embedding(1, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): RobertaEncoder(
    (layer): ModuleList(
      (0-11): 12 x RobertaLayer(
        (attention): RobertaAttention(
          (self): RobertaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): RobertaSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): RobertaIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): RobertaOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): RobertaPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
INFO:root:roberta-base_D
INFO:root:The number of parameters: 124645632
INFO:root:5 sample adjacenecy matrices
--------------------------------------------------
Text: Undirected graph with with cycle, max diameter 2.
Results: [50, 9, 0, 3, 5, 42, True]
--------------------------------------------------
--------------------------------------------------
Text: Undirected graph with min degree 1, 5 nodes, 2 connected component, 3 edges, without cycle, max diameter 2, max degree 2.
Results: [7, 9, 0, 4, 2, 3, True]
--------------------------------------------------
--------------------------------------------------
Text: Undirected graph with 22 nodes, max degree 9, max diameter 4.
Results: [23, 55, 0, 10, 4, 2, True]
--------------------------------------------------
--------------------------------------------------
Text: Undirected graph with min degree 1, max diameter 5.
Results: [50, 18, 0, 6, 4, 36, True]
--------------------------------------------------
--------------------------------------------------
Text: Undirected graph with max diameter 5, 1 connected component, max degree 11, 23 nodes.
Results: [23, 64, 0, 9, 3, 1, True]
--------------------------------------------------

INFO:root:Elapsed [0:00:10], Iteration [1/100]:
l_D/R: 10.82, l_D/F: 3.67, l_D: -5.10, l_G: -3.80, l_R: 0.00, l_R/N: 0.00, l_R/M: 0.00
property_match: 0.30, closeness: 0.43, n_match: 0.34, m_match: 0.07, min_deg_match: 0.01, max_deg_match: 0.21, diam_match: 0.25, cc_match: 0.22, cycle_match: 0.87
--------------------------------------------------
Nodes in (0, 5]: 25
property_match: 0.48, closeness: 0.66, n_match: 0.72, m_match: 0.20, min_deg_match: 0.00, max_deg_match: 0.40, diam_match: 0.53, cc_match: 0.53, cycle_match: 0.67
Nodes in (5, 10]: 86
property_match: 0.39, closeness: 0.54, n_match: 0.83, m_match: 0.10, min_deg_match: 0.00, max_deg_match: 0.16, diam_match: 0.32, cc_match: 0.38, cycle_match: 0.87
Nodes in (10, 25]: 94
property_match: 0.34, closeness: 0.47, n_match: 0.50, m_match: 0.07, min_deg_match: 0.02, max_deg_match: 0.24, diam_match: 0.25, cc_match: 0.44, cycle_match: 0.89
Nodes in (25, 50]: 295
property_match: 0.24, closeness: 0.37, n_match: 0.11, m_match: 0.04, min_deg_match: 0.01, max_deg_match: 0.21, diam_match: 0.20, cc_match: 0.05, cycle_match: 0.88
--------------------------------------------------
