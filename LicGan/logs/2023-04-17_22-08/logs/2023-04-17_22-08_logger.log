INFO:root:Namespace(lm_model='bert-base-uncased', N=50, max_len=128, z_dim=8, mha_dim=768, n_heads=8, gen_dims=[[128, 256, 768], [512, 256, 128]], disc_dims=[[128, 128], [512, 768], [512, 256, 128]], lambda_gp=2, lambda_wgan=1, post_method='sigmoid', batch_size=128, num_epochs=100, g_lr=0.0002, d_lr=0.0002, r_lr=0.001, dropout=0.0, n_critic=5, test_epochs=100, num_workers=1, mode='test', data_dir='data', saving_dir='results/2023-04-17_22-08', model_save_step=20, lr_update_step=1000, restore_G='/home/abisheks/10708-Project/MolGAN-PyTorch/results/2023-04-17_20-13/models/20-G.ckpt', restore_D='/home/abisheks/10708-Project/MolGAN-PyTorch/results/2023-04-17_20-13/models/20-D.ckpt', restore_R='/home/abisheks/10708-Project/MolGAN-PyTorch/results/2023-04-17_20-13/models/20-R.ckpt', name='symm_fcn_simple_ds_rew', log_dir='results/2023-04-17_22-08/logs', model_dir='results/2023-04-17_22-08/models')
INFO:root:Generator(
  (activation_f): ReLU()
  (multi_dense_layer): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=8, out_features=128, bias=True)
      (1): Dropout(p=0.0, inplace=False)
      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=256, bias=True)
      (5): Dropout(p=0.0, inplace=False)
      (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=768, bias=True)
      (9): Dropout(p=0.0, inplace=False)
      (10): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (11): ReLU()
    )
  )
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (multi_dense_layer_2): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=768, out_features=512, bias=True)
      (1): Dropout(p=0.0, inplace=False)
      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=256, bias=True)
      (5): Dropout(p=0.0, inplace=False)
      (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=128, bias=True)
      (9): Dropout(p=0.0, inplace=False)
      (10): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (11): ReLU()
    )
  )
  (adjM_layer): Linear(in_features=128, out_features=2500, bias=True)
  (dropoout): Dropout(p=0.0, inplace=False)
)
INFO:root:G
INFO:root:The number of parameters: 3478468
INFO:root:Discriminator(
  (activation_f): ReLU()
  (multi_dense_layer): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=50, out_features=128, bias=True)
      (1): Dropout(p=0.0, inplace=False)
      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Dropout(p=0.0, inplace=False)
      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
    )
  )
  (multi_dense_layer_2): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=6400, out_features=512, bias=True)
      (1): Dropout(p=0.0, inplace=False)
      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=768, bias=True)
      (5): Dropout(p=0.0, inplace=False)
      (6): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
    )
  )
  (mha): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
  )
  (multi_dense_layer_3): MultiDenseLayer(
    (linear_layer): Sequential(
      (0): Linear(in_features=768, out_features=512, bias=True)
      (1): Dropout(p=0.0, inplace=False)
      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=256, bias=True)
      (5): Dropout(p=0.0, inplace=False)
      (6): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=128, bias=True)
      (9): Dropout(p=0.0, inplace=False)
      (10): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (11): ReLU()
    )
  )
  (output_layer): Linear(in_features=128, out_features=1, bias=True)
)
INFO:root:D
INFO:root:The number of parameters: 6619649
INFO:root:RewardNet(
  (node_cnt): Sequential(
    (0): Linear(in_features=2500, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=1, bias=True)
    (5): Sigmoid()
  )
)
INFO:root:R
INFO:root:The number of parameters: 673281
INFO:root:BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0-11): 12 x BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
INFO:root:bert-base-uncased
INFO:root:The number of parameters: 109482240
INFO:root:5 sample adjacenecy matrices
--------------------------------------------------
Text: Generate undirected graph with 25 nodes and 27 edges.
Num Nodes: 50 | Num Edges: 1250 | Edg: 52.86497116088867 | Node: 0.5459470748901367
--------------------------------------------------
--------------------------------------------------
Text: Generate undirected graph with 50 nodes and 72 edges.
Num Nodes: 50 | Num Edges: 1250 | Edg: 130.03619384765625 | Node: 0.9573009610176086
--------------------------------------------------
--------------------------------------------------
Text: Generate undirected graph with 50 nodes and 69 edges.
Num Nodes: 50 | Num Edges: 1250 | Edg: 171.61770629882812 | Node: 0.9999980926513672
--------------------------------------------------
--------------------------------------------------
Text: Generate undirected graph with 50 nodes and 76 edges.
Num Nodes: 50 | Num Edges: 1250 | Edg: 185.12677001953125 | Node: 1.0
--------------------------------------------------
--------------------------------------------------
Text: Generate undirected graph with 50 nodes and 74 edges.
Num Nodes: 50 | Num Edges: 1250 | Edg: 102.22528839111328 | Node: 0.9739258289337158
--------------------------------------------------

INFO:root:Elapsed [0:00:48], Iteration [1/100]:
l_D/R: -3.46, l_D/F: -15.76, l_D: -9.07, l_G: 15.76, l_R: 0.00, l_R/N: 0.00, l_R/M: 0.00
property_match: 0.12, closeness: 0.12
